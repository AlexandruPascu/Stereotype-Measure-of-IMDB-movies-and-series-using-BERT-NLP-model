{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz5T5YzMlZHl"
      },
      "outputs": [],
      "source": [
        "# MIT License\n",
        "\n",
        "# Copyright (c) 2022 Alexandru Pascu and Stefano Li Pira\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in all\n",
        "# copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "# SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Measuring the Stereotype of IMDB movies and series using a BERT NLP model Version: 1.0 Date: 2022-10-7**\n",
        "\n",
        "> This notebook contains complete code to fine-tune the transformers language model BERT to perform categorization of movies and series into a label especially the probability of them being a Stereotype title. We will use BERT to categorize titles from IMDB datasets in stereotypes and non-stereotypes (the catergories can be easily changed for other usecases). Data needed for this matter is the users keywords of titles and plots (we are excluding game-show, talk-show, reality-tv and filtering for drama and comedy. We are including only episodes with at least 1 rating for relevance). BigQuery has been used for some data cleaning purposes.\n",
        "\n",
        "**In this notebook, you will:**\n",
        "\n",
        "> Download and load a dataset of series title episodes with various information about them such as actors, production, ratings, plots, keywords and others. Based on the users existing keywords titles will be put into a category (easily changeble) and used for training and then labeling others (keywords can be changed and use in different ways). The dataset was created from the Goodreads dataset. Load a BERT model from the Transformers Library, build your own model by combining BERT with a classifier. Train your own model on a GPU, fine-tuning BERT on the training dataset. Use the trained model to classify titles as Stereotype using plots. Use the predicted categorization probabilities to run regressions and find insights. You can use this notebook on your own data from other domains and categories, by ensuring that your input and prediction datasets have the same structure and column names as those provided with this notebook. Alternatively the later can be modified as well to suit your use-case."
      ],
      "metadata": {
        "id": "AmvNrYYTlwhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**File Paths**\n",
        "\n",
        "Mount your drive folder in the colab.\n",
        "\n",
        "The folllowing cell should be modified if you are using different input/prediction files or if you have changed the folder name."
      ],
      "metadata": {
        "id": "N7ntP37u97MN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibaHl6g7OBjB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install and load libraries**"
      ],
      "metadata": {
        "id": "mCswiXLL-u_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the Cinemagoer API\n",
        "!pip install cinemagoer\n",
        "# install the Simple Transformers library\n",
        "!pip install simpletransformers"
      ],
      "metadata": {
        "id": "BMlC8dHv-xGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo0vernMqPTH"
      },
      "outputs": [],
      "source": [
        "# General Packages #\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# CSV handling library\n",
        "import csv\n",
        "\n",
        "# Cinemagoer library\n",
        "from imdb import Cinemagoer, IMDbError\n",
        "\n",
        "# The classification Model used to label data\n",
        "from simpletransformers.classification import ClassificationModel\n",
        "\n",
        "# TQDM to Show Progress Bars #\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "\n",
        "# SKLearn libraries for splitting sample and validation\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score\n",
        "from sklearn.metrics import recall_score, f1_score, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Matplot and SkLearn libraries for doing regressions\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Additional Libraries that we are using only in this notebook\n",
        "import torch\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cinemagoer API**\n",
        "\n",
        "The following code blocks are to help with understanding better what data Cinemagoer (https://cinemagoer.github.io/) is capable of retrivieving from IMDB (https://www.imdb.com/). We also provide access to how we handled our data collection process."
      ],
      "metadata": {
        "id": "2BGZD2SLAyfN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPqEZZEBoQJL"
      },
      "outputs": [],
      "source": [
        "# create an instance of the Cinemagoer class\n",
        "ia = Cinemagoer()\n",
        "\n",
        "# get a movie and print its director(s)\n",
        "the_matrix = ia.get_movie('0720229')\n",
        "for director in the_matrix['directors']:\n",
        "  print(director['name'])\n",
        "\n",
        "# show all information that are currently available for a movie\n",
        "print(sorted(the_matrix.keys()))\n",
        "\n",
        "# show all information sets that can be fetched for a movie\n",
        "print(ia.get_movie_infoset())\n",
        "\n",
        "# update a Movie object with more information\n",
        "ia.update(the_matrix, ['technical'])\n",
        "\n",
        "# show which keys were added by the information set\n",
        "print(the_matrix.infoset2keys['technical'])\n",
        "\n",
        "# print one of the new keys\n",
        "print(the_matrix.get('tech'))\n",
        "\n",
        "# print different information\n",
        "print(the_matrix.get('plot'))\n",
        "print(the_matrix.get('plot outline'))\n",
        "print(the_matrix.get('synopsis'))\n",
        "print(the_matrix.get('keywords'))\n",
        "print(the_matrix.get('title'))\n",
        "print(the_matrix.get('production companies'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following blocks you can use the code to get data neede for the title codes you want. We were interested in plots, keywords, production companies, and countries. "
      ],
      "metadata": {
        "id": "rqMRp-tGwDRT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQq0uYZj31K6"
      },
      "outputs": [],
      "source": [
        "# initialize the header row \n",
        "headers = ['production companies', 'plot', 'plot outline', 'synopsis',\n",
        "           'keywords', 'countries', 'title_code']\n",
        "# initialize data\n",
        "data = ['', '', '', '', '', '', '']\n",
        "\n",
        "# add the csv file with the title codes to get data on, in colab files on left\n",
        "file = open(\"title_codes.csv\")\n",
        "csvreader = csv.reader(file)\n",
        "header = next(csvreader)\n",
        "rows = []\n",
        "# add all the codes in the rows variable\n",
        "for row in csvreader:\n",
        "  rows.append(row)\n",
        "file.close()\n",
        "\n",
        "# creata a folder to store the data from imdb\n",
        "f = open('titles_data.csv', 'w', encoding='UTF8', newline='')\n",
        "writer = csv.writer(f)\n",
        "writer.writerow(headers)\n",
        "\n",
        "# iterate through all the the titles codes and getting data interested in\n",
        "for x in rows:\n",
        "  try:\n",
        "    x_string = ' '.join(map(str, x))\n",
        "    the_matrix = ia.get_movie(x_string)\n",
        "    if the_matrix.get('production companies') is None:\n",
        "      data[0] = \"NULL\"\n",
        "    else:\n",
        "      data[0] = the_matrix.get('production companies')\n",
        "    if the_matrix.get('plot') is None:\n",
        "      data[1] = \"NULL\"\n",
        "    else:\n",
        "      data[1] = the_matrix.get('plot')[0]\n",
        "    if the_matrix.get('plot outline') is None:\n",
        "      data[2] = \"NULL\"\n",
        "    else:\n",
        "      data[2] = the_matrix.get('plot outline')\n",
        "    if the_matrix.get('synopsis') is None:\n",
        "      data[3] = \"NULL\"\n",
        "    else:\n",
        "      data[3] = the_matrix.get('synopsis')\n",
        "    # keywords weren't adding correctly; see below the solution for getting them\n",
        "    if the_matrix.get('keywords') is None:\n",
        "      data[4] = \"NULL\"\n",
        "    else:\n",
        "      data[4] = the_matrix.get('keywords')\n",
        "    if the_matrix.get('countries') is None:\n",
        "      data[5] = \"NULL\"\n",
        "    else:\n",
        "      data[5] = the_matrix.get('countries')\n",
        "    data[6] = \"tt\" + x_string\n",
        "    writer.writerow(data)\n",
        "  except IMDbError as e:\n",
        "    print(e)\n",
        "    continue\n",
        "\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We discovered a problem in the implementation of the API hence we used the following code to get the keyowrds of the title as well. They require a special approach."
      ],
      "metadata": {
        "id": "P8SGemvxwLvU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxlK9HncUboo"
      },
      "outputs": [],
      "source": [
        "# initialize the header row \n",
        "headers = ['title_codes', 'keywords_title']\n",
        "# initialize data\n",
        "data = ['', '']\n",
        "\n",
        "# add the csv file with the title codes to get data on, in colab files on left\n",
        "file = open(\"title_codes.csv\")\n",
        "csvreader = csv.reader(file)\n",
        "rows = []\n",
        "# add all the codes in the rows variable\n",
        "for row in csvreader:\n",
        "  rows.append(row)\n",
        "file.close()\n",
        "\n",
        "# creata a folder to store the data from imdb\n",
        "f = open('titles_keywords.csv', 'w', encoding='UTF8', newline='')\n",
        "writer = csv.writer(f)\n",
        "writer.writerow(headers)\n",
        "\n",
        "# get all the keywords of the titles\n",
        "for x in rows:\n",
        "  x_string = ' '.join(map(str, x))\n",
        "  data[0] = \"tt\" + x_string\n",
        "  try:\n",
        "    title = ia.get_movie(x_string, info='keywords')\n",
        "    try:\n",
        "      data[1] = title['keywords']\n",
        "    except:\n",
        "      data[1] = \"NULL\"\n",
        "  except IMDbError as e:\n",
        "    print(e)\n",
        "    continue\n",
        "  writer.writerow(data)\n",
        "f.close"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we wanted to have the opposite approach, we had a couple of specific keywords area that we were looking for similar ones in the imdb database. Afterwards we used them to get the title codes associated with all these keywords."
      ],
      "metadata": {
        "id": "5mA5Cz4EwRG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creata a folder to store the data from imdb\n",
        "f = open('tiles_from_keywords.csv', 'w', encoding='UTF8', newline='')\n",
        "writer = csv.writer(f)\n",
        "data = ['', '']\n",
        "\n",
        "# compute the list of 'categories' that you want to get similar keywords for\n",
        "keywordsList = ['stereotype', 'sexism', 'homophobic',\n",
        "                'racism', 'discrimination']\n",
        "# iterate through your list and get similar keywords\n",
        "for keywordIndividual in keywordsList:\n",
        "  keywords = ia.search_keyword(keywordIndividual)\n",
        "  print(keywords)\n",
        "  # iterate through the list of similar keywords and get title codes for them\n",
        "  for keywwordsSimilar in keywords:\n",
        "    movies = ia.get_keyword(keywwordsSimilar)\n",
        "    # save all the titles in a csv file\n",
        "    for title in movies:\n",
        "      data[0] = 'tt' + ''.join(title.movieID)\n",
        "      data[1] = keywwordsSimilar\n",
        "      writer.writerow(data)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "ZUtq3F2fNsT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For using them to train the model as well, get their plots and all of the keywords they have for each title code found earlier."
      ],
      "metadata": {
        "id": "F67i3qe4wWFK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeXLHePTyAtQ"
      },
      "outputs": [],
      "source": [
        "# initialize the header row \n",
        "headers = ['title_codes', 'keywords_title', 'plot']\n",
        "# initialize data\n",
        "data = ['', '', '']\n",
        "\n",
        "# open the file containing the title codes and add them to a variable\n",
        "file = open(\"tiles_from_keywords.csv\")\n",
        "csvreader = csv.reader(file)\n",
        "rows = []\n",
        "for row in csvreader:\n",
        "  rows.append(row)\n",
        "file.close()\n",
        "\n",
        "# creata a folder to store the data from imdb\n",
        "f = open('plots_and_keywords_extra_titles.csv', 'w',\n",
        "         encoding='UTF8', newline='')\n",
        "writer = csv.writer(f)\n",
        "writer.writerow(headers)\n",
        "\n",
        "# get all the keywords and plots of the titles\n",
        "for x in rows:\n",
        "  data[0] = ''.join(map(str, x))\n",
        "  x_string = data[0][2:]\n",
        "  try:\n",
        "    title = ia.get_movie(x_string, info=['keywords', 'plot'])\n",
        "    try:\n",
        "      data[1] = title['keywords']\n",
        "    except:\n",
        "      data[1] = \"NULL\"\n",
        "    try:\n",
        "      data[2] = title['plot']\n",
        "    except:\n",
        "      data[2] = \"NULL\"\n",
        "  except IMDbError as e:\n",
        "    print(e)\n",
        "    continue\n",
        "  writer.writerow(data)\n",
        "f.close"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare data for the ML model**\n",
        "\n",
        "From now on the focus starts to be on preparing the data for the ML model. Preparing the labels, the needed columns from our dataset (namely plots, keywords, title codes) and associating the Label Stereotype or not."
      ],
      "metadata": {
        "id": "sfeXPwZwur1H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIbhKqM_GtzN"
      },
      "outputs": [],
      "source": [
        "# have a list of keywords that you want to find similars\n",
        "labelList = ['stereotype', 'sexism', 'homophobic', 'racism', 'discrimination']\n",
        "keywords = ([[], [], [], [], []])\n",
        "\n",
        "# iterate through the keywords list to get the similar ones\n",
        "for i in range(len(labelList)):\n",
        "  print(labelList[i])\n",
        "  keywords[i] = ia.search_keyword(labelList[i])\n",
        "  print(keywords[i])\n",
        "print(keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the data that you (in the csv format, with each coloumn in one file - make sure the order is preserved. Alternatively use python to extract the needed information, we used BigQuery with SQL to do this job) have on Drive and load it in dataframes."
      ],
      "metadata": {
        "id": "VrWVqRkNvLmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGBMtme20EVS"
      },
      "outputs": [],
      "source": [
        "# making dataframes\n",
        "df = pd.read_csv('/content/drive/MyDrive/titles.csv', header = 0, \n",
        "                 delimiter=\"\\t\", quoting= 3, encoding='utf-8')\n",
        "df_keywords = pd.read_csv('/content/drive/MyDrive/keywords.csv',header = 0,\n",
        "                          delimiter=\"\\t\", quoting= 3, encoding='utf-8')\n",
        "df_plot = pd.read_csv('/content/drive/MyDrive/plot.csv', header = 0,\n",
        "                      delimiter=\"\\t\", quoting= 3, encoding='utf-8')\n",
        "\n",
        "# combining all relevant data into one dataframe\n",
        "df['keywords'], df['plot'] = [df_keywords, df_plot]\n",
        "\n",
        "# output the dataframe\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making dataframe \n",
        "df_extra = pd.read_csv('/content/drive/MyDrive/extra_title.csv', header = 0,\n",
        "                       delimiter=\"\\t\", quoting= 3, encoding='utf-8')\n",
        "df_extra_keywords = pd.read_csv('/content/drive/MyDrive/extra_keywords.csv',\n",
        "                                header = 0, delimiter=\"\\t\", quoting= 3,\n",
        "                                encoding='utf-8')\n",
        "df_extra_plot = pd.read_csv('/content/drive/MyDrive/extra_plot.csv', header = 0,\n",
        "                            delimiter=\"\\t\", quoting= 3, encoding='utf-8')\n",
        "\n",
        "# combining all relevant data into one dataframe\n",
        "df_extra['keywords'], df_extra['plot'] = [df_extra_keywords, df_extra_plot]\n",
        "\n",
        "# output the dataframe\n",
        "display(df_extra)"
      ],
      "metadata": {
        "id": "OnpvT9FzBuOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the keywords in the big areas you are interested in and label the data with them."
      ],
      "metadata": {
        "id": "xV1Fk7Rovu1f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnGWDNR5RXPN"
      },
      "outputs": [],
      "source": [
        "# initialize the pattern list with a list for each keyword area looked for\n",
        "pattern = ([[], [], [], [], []])\n",
        "\n",
        "# transform the similar keywords got from imdb in order to work with them\n",
        "for i in range(len(keywords)):\n",
        "  pattern[i] = '|'.join(keywords[i])\n",
        "  print(pattern[i])\n",
        "print(pattern)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate through the 5 areas of all the keywords and save in the dataframe\n",
        "# the presence with True and False\n",
        "for i in range(len(keywords)):\n",
        "  df[i] = df.keywords.str.contains(pattern[i])\n",
        "display(df)"
      ],
      "metadata": {
        "id": "6fr1LjTJL0Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psAUiZ8zPKVk"
      },
      "outputs": [],
      "source": [
        "# iterate through the list of True and False and label them with 1 for True\n",
        "# 0 for otherwise and rename the columns of the dataframe to represent the \n",
        "# keyword area\n",
        "for i in range(len(keywords)):\n",
        "  df.loc[df[i] == True, i] = 1\n",
        "  df.loc[df[i] == False, i] = 0\n",
        "\n",
        "df.rename(columns={0: 'stereotype', 1: 'sexism', 2: 'homophobic', \n",
        "                   3: 'racism', 4: 'discrimination'}, inplace = True)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do the same with the extra titles\n",
        "for i in range(len(keywords)):\n",
        "  df_extra.loc[df_extra[i] == True, i] = 1\n",
        "  df_extra.loc[df_extra[i] == False, i] = 0\n",
        "\n",
        "df_extra.rename(columns={0: 'stereotype', 1: 'sexism', 2: 'homophobic',\n",
        "                         3: 'racism', 4: 'discrimination'}, inplace = True)\n",
        "display(df_extra)"
      ],
      "metadata": {
        "id": "EWLv3gtATGeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "584t1P7qTqbz"
      },
      "outputs": [],
      "source": [
        "# get stats on all the keywords area looked for in the initial data\n",
        "print(df['stereotype'].value_counts())\n",
        "print(df['sexism'].value_counts())\n",
        "print(df['homophobic'].value_counts())\n",
        "print(df['racism'].value_counts())\n",
        "print(df['discrimination'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do the same with the extra titles\n",
        "print(df_extra['stereotype'].value_counts())\n",
        "print(df_extra['sexism'].value_counts())\n",
        "print(df_extra['homophobic'].value_counts())\n",
        "print(df_extra['racism'].value_counts())\n",
        "print(df_extra['discrimination'].value_counts())"
      ],
      "metadata": {
        "id": "cxr1i_dFTRQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqVSOgPcmMSf"
      },
      "outputs": [],
      "source": [
        "# create a new label column\n",
        "df['Type'] = ''\n",
        "# label the titles with the keyword area that it has\n",
        "for i in ['stereotype', 'sexism', 'homophobic', 'racism', 'discrimination']:\n",
        "  df.loc[df[i] == 1, 'Type'] = df['Type'] + ' ' + i\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do the same with the extra titles\n",
        "df_extra['Type'] = ''\n",
        "for i in ['stereotype', 'sexism', 'homophobic', 'racism', 'discrimination']:\n",
        "  df_extra.loc[df_extra[i] == 1, 'Type'] = df_extra['Type'] + ' ' + i\n",
        "display(df_extra)"
      ],
      "metadata": {
        "id": "amWCpMl_Tb6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojt0PimUuqns"
      },
      "outputs": [],
      "source": [
        "# create the join dataframe having the initial data\n",
        "joint_data = df.append([df_extra])\n",
        "# separate the records that have at least one label from the joint data\n",
        "data_types_joint_data = joint_data.loc[joint_data['Type'] != '']\n",
        "# separate the records that have no label from the initial data\n",
        "data_nontypes = joint_data[df['Type'] == '']\n",
        "# randomly select records with no labels\n",
        "# you have to add up the records with label and change n with that number\n",
        "data_nontypes = data_nontypes.sample(n = 3688) \n",
        "# have a dataframe with both the records with and without label selected\n",
        "training_joint_data = data_types_joint_data.append([data_nontypes])\n",
        "display(training_joint_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWkF8v_nyVyD"
      },
      "outputs": [],
      "source": [
        "# get stats on all the keywords area looked for in the joint data\n",
        "print(joint_data['stereotype'].value_counts())\n",
        "print(joint_data['sexism'].value_counts())\n",
        "print(joint_data['homophobic'].value_counts())\n",
        "print(joint_data['racism'].value_counts())\n",
        "print(joint_data['discrimination'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the labels and the plots ready for training the model."
      ],
      "metadata": {
        "id": "ykzx1vYRQi_Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y41nnvZfv1gW"
      },
      "outputs": [],
      "source": [
        "# transform the label column from the joint data in a list with the values\n",
        "# if there is no value save it as 0 otherwise as 1\n",
        "stereotype_encoding_joint_data = training_joint_data['Type'].values.tolist()\n",
        "for i in range(len(stereotype_encoding_joint_data)):\n",
        "  if stereotype_encoding_joint_data[i] == '':\n",
        "    stereotype_encoding_joint_data[i] = 0\n",
        "  else:\n",
        "    stereotype_encoding_joint_data[i] = 1\n",
        "print(stereotype_encoding_joint_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL_q1bujwMKm"
      },
      "outputs": [],
      "source": [
        "# in case you want to do multi label classification you need a matrix of them\n",
        "class_array_joint_data = training_joint_data[['stereotype', 'sexism', \n",
        "                          'homophobic', 'racism', 'discrimination']].to_numpy()\n",
        "for row in class_array_joint_data:\n",
        "  if 1 in row:\n",
        "    row = 1\n",
        "  else:\n",
        "    row = 0\n",
        "print(class_array_joint_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjVmSh0Bwymj"
      },
      "outputs": [],
      "source": [
        "# Store Data in Lists for Text Classification\n",
        "IDs = np.array(training_joint_data['titleCodes'].values.tolist())\n",
        "Plot_Text = training_joint_data['plot'].values.tolist()\n",
        "Classes = stereotype_encoding_joint_data\n",
        "print(Classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train a NLP Model**\n",
        "\n",
        "Change the parameters of the Bert Classification model according to your dataset and needs. For best results we tested several different settings and in the a number of 3 epochs and a batch size of 16 gave us the best results on Colab with a GPU running. This is because Bert is already pre-trained on an immense dataset of books and wikipedia articles and you just need to fine-tune it for your usecase.\n",
        "\n",
        "See for guidance and resources: https://simpletransformers.ai/docs/installation/"
      ],
      "metadata": {
        "id": "s_ofwDRLvShN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RwnPlCbO9dv"
      },
      "outputs": [],
      "source": [
        "# Loop through K Folds and Repeat Cross Validation\n",
        "NUM_OF_SPLITS = 5\n",
        "KFoldSplitter = StratifiedKFold(n_splits = NUM_OF_SPLITS, shuffle = True,\n",
        "                                random_state = 1)\n",
        "        \n",
        "for train_i, test_i in tqdm_notebook(KFoldSplitter.split(Plot_Text, Classes), \n",
        "                                            desc = 'Cross-Validating',\n",
        "                                            leave = False,\n",
        "                                            total = NUM_OF_SPLITS):\n",
        "      \n",
        "  # Select Rows in Data Based on Indexes [train_i, test_i]\n",
        "  Y = np.array(Classes)\n",
        "\n",
        "  Plot_Text_Array = np.array(Plot_Text)\n",
        "\n",
        "  train_X, test_X = Plot_Text_Array[train_i], Plot_Text_Array[test_i]\n",
        "  train_y, test_y = Y[train_i], Y[test_i]\n",
        "  Train_IDs, Test_IDs = IDs[train_i], IDs[test_i]\n",
        "\n",
        "  # Create Training Data in Paired Format (Nessesary for Transformers)\n",
        "  TrainingDataframe = list(zip( list(train_X), list(train_y)))\n",
        "  TestDataframe = list(zip( list(test_X), list(test_y)))\n",
        "\n",
        "  train_df = pd.DataFrame(TrainingDataframe)\n",
        "  train_df.columns = [\"text\", \"labels\"]\n",
        "\n",
        "model = ClassificationModel(\"bert\", \"bert-base-uncased\", use_cuda = True,\n",
        "                            num_labels=2,                                \n",
        "                                    args={'num_train_epochs':3,\n",
        "                                          'train_batch_size':16,\n",
        "                                          'overwrite_output_dir': True,\n",
        "                                          'use_early_stopping':True,\n",
        "                                          'do_lower_case':True, \n",
        "                                          'silent':True,\n",
        "                                          'no_cache':True, \n",
        "                                          'no_save':True,\n",
        "                                          \"regression\": False}\n",
        "                                    )\n",
        "\n",
        "# Train the Model\n",
        "model.train_model(train_df)\n",
        "\n",
        "# Predict on Holdout Sample\n",
        "predictions, raw_outputs = model.predict( list(test_X) )\n",
        "\n",
        "# Store Output\n",
        "id_s = id_s + list(Test_IDs)\n",
        "y_actual = y_actual + list(test_y)\n",
        "y_predicted = y_predicted + list(predictions)\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12CPoP4_V9rV"
      },
      "outputs": [],
      "source": [
        "# Compute the Share of AI Patents\n",
        "Share = np.round(np.mean(y_predicted), 3)\n",
        "\n",
        "# Calculate Model Performance Metrics\n",
        "Accuracy = accuracy_score(y_actual, y_predicted)\n",
        "ROC = roc_auc_score(y_actual, y_predicted)\n",
        "Precision = precision_score(y_actual, y_predicted)\n",
        "Recall = recall_score(y_actual, y_predicted)\n",
        "F1 = f1_score(y_actual, y_predicted)\n",
        "CM = confusion_matrix(y_actual, y_predicted)\n",
        "\n",
        "FN = np.round(CM[0][0]/(CM[0][0] + CM[1][0]), 3)\n",
        "FP = np.round(CM[0][1]/(CM[0][1] + CM[1][1]), 3)\n",
        "TN = np.round(CM[1][0]/(CM[0][0] + CM[1][0]), 3)\n",
        "TP = np.round(CM[1][1]/(CM[0][1] + CM[1][1]), 3)\n",
        "\n",
        "# Add Classification Performance Metrics to List\n",
        "RESULTS.append(['bert', Share, TP, FN, FP, TN,\n",
        "                                            np.round(Accuracy, 3),\n",
        "                                            np.round(ROC, 3),\n",
        "                                            np.round(Precision, 3),\n",
        "                                            np.round(Recall, 3),\n",
        "                                            np.round(F1, 3)])\n",
        "\n",
        "# Add Classification Results to List \n",
        "Classified_Values.append(list(zip(len(id_s)*['bert'],\n",
        "                                  id_s, y_actual, y_predicted)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1DC1-M-UxJq"
      },
      "outputs": [],
      "source": [
        "# Convert List to Dataframe\n",
        "RESULTS_TABLE = pd.DataFrame(RESULTS, columns = [\"Name\", \"Share\", \n",
        "                                                 \"True-Positives\", \n",
        "                                                 \"False-Negatives\",\n",
        "                                                 \"False-Positives\", \n",
        "                                                 \"True-Negatives\",\n",
        "                                                 \"Accuracy\", \"AUC\", \n",
        "                                                 \"Precision\", \"Recall\", \"F1\"] )\n",
        "\n",
        "RESULTS_TABLE[\"Type\"] = \"Transformer\"\n",
        "RESULTS_TABLE = RESULTS_TABLE[[\"Name\", \"Type\", \"Share\", \"True-Positives\", \n",
        "                               \"False-Negatives\", \"False-Positives\", \n",
        "                               \"True-Negatives\",\"Accuracy\", \"AUC\", \n",
        "                               \"Precision\", \"Recall\", \"F1\"]]\n",
        "\n",
        "\n",
        "\n",
        "# Output Results\n",
        "RESULTS_TABLE.sort_values(\"Accuracy\", ascending = False).to_csv('Transformer Classification Model Performance.csv')\n",
        "\n",
        "# Display Results -- Out of Sample (Holdout) prediction -- Sorted by Accuracy\n",
        "RESULTS_TABLE.sort_values(\"Accuracy\", ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnHPJ3u8hbHQ"
      },
      "outputs": [],
      "source": [
        "# Output Classification Results for Training Dataset\n",
        "# -- PREDICTED VALUES -- Out Of Sample (Holdout) Prediction\n",
        "\n",
        "for i in range(0,len(Classified_Values), 1):\n",
        "\n",
        "  Temp = pd.DataFrame(  Classified_Values[i],\n",
        "                        columns = ['Model', 'id', 'Actual', 'Predicted'] )\n",
        "  \n",
        "  if i == 0: \n",
        "    name = Temp.head(1)['Model'][0]\n",
        "    Temp = Temp[['id', 'Actual', 'Predicted']]\n",
        "    Temp.columns = ['id', 'Actual', name]\n",
        "    Final = Temp\n",
        "\n",
        "  else: \n",
        "\n",
        "    name = Temp.head(1)['Model'][0]\n",
        "    Temp = Temp[['id', 'Predicted']]\n",
        "    Temp.columns = ['id', name]\n",
        "\n",
        "    Final = Final.merge(Temp, on = ['id'])\n",
        "\n",
        "# Save DataFrame # \n",
        "Final.to_csv(\"./Transformer Classification Results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predictions on new data**\n",
        "\n",
        "Use the just fine-tuned bert model to test the clasifications (labelling) on a couple of plots and then on your dataset."
      ],
      "metadata": {
        "id": "83wuZfDmvW-n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsNw5CY8y7op"
      },
      "outputs": [],
      "source": [
        "# try different plots to see what the model is predicting them\n",
        "# change or add plots in the list to experience with others\n",
        "predictions, raw_outputs = model.predict([\"With the help of a German bounty-hunter, a freed slave sets out to rescue his wife from a brutal plantation-owner in Mississippi.\", \"Everyone knows that lions live in Africa and tigers live in Asia, right? Wrong. The world's last population of Asiatic lions live in northern India, 200 miles away from the nearest tigers. How did these two apex predators choose their different habitats?\", \"Carol Danvers becomes one of the universe's most powerful heroes when Earth is caught in the middle of a galactic war between two alien races.\", \"A pickup game with the family-owned pizza place across the street leads to Leela being the first female Blernsball player, but she has to struggle to avoid being the worst player ever.\", \"T'Challa, heir to the hidden but advanced kingdom of Wakanda, must step forward to lead his people into a new future and must confront a challenger from his country's past.\"])\n",
        "print(predictions, raw_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the csv in a dataframe \n",
        "df_to_predict = pd.read_csv('/content/drive/MyDrive/to_predict.csv', header = 0,\n",
        "                            encoding='utf-8')"
      ],
      "metadata": {
        "id": "-fC7Ik96qcIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into smaller chunks if it is too big\n",
        "# change the start and the end accordingly\n",
        "start = 0\n",
        "end = 16000\n",
        "predict_list = df_to_predict['plot'].iloc[start:end].values.tolist()"
      ],
      "metadata": {
        "id": "nkmdQ2RCsOPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz6dC_gV8c6H"
      },
      "outputs": [],
      "source": [
        "# initialize numpy arrays for predictions\n",
        "predict_outcome = np.array([])\n",
        "predict_nonstereotype = np.array([])\n",
        "predict_stereotype = np.array([])\n",
        "# have the outcome, the stereotype and non stereotype measures\n",
        "for i in range(len(predict_list)):\n",
        "  # run the model and distribute the correct values to the right arrays\n",
        "  predictions, raw_outputs = model.predict([predict_list[i]])\n",
        "  predict_outcome = np.append(predict_outcome, predictions)\n",
        "  predict_nonstereotype = np.append(predict_nonstereotype, raw_outputs[0][0])\n",
        "  predict_stereotype = np.append(predict_stereotype, raw_outputs[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cWNzxW4Kq_p"
      },
      "outputs": [],
      "source": [
        "# select the chunk you're working with from those you want to run the model on\n",
        "temp = df_to_predict.iloc[start:end]\n",
        "# add the predictions to the dataframe\n",
        "temp['Prediction'] = predict_outcome.tolist()\n",
        "temp['NonStereotype'] = predict_nonstereotype.tolist()\n",
        "temp['Stereotype'] = predict_stereotype.tolist()\n",
        "# save the predictions in a csv on your drive\n",
        "temp.to_csv('/content/drive/MyDrive/predicted.csv', encoding='utf-8')\n",
        "print(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regressions on the stereotype measures**"
      ],
      "metadata": {
        "id": "LeHhnV_qY0G-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the csv in a dataframe \n",
        "df_popularity = pd.read_csv('/content/drive/MyDrive/popularity_regression.csv',\n",
        "                            header = 0, encoding='utf-8')\n",
        "# randomise the sample order\n",
        "df_popularity = df_popularity.sample(frac = 1).reset_index(drop = True)\n",
        "display(df_popularity)"
      ],
      "metadata": {
        "id": "J2sQ67poYzoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup the Linear Regression model and select the independent variables\n",
        "X = df_popularity[['Stereotype', 'NonStereotype', 'prediction']]\n",
        "# select the dependent variables\n",
        "y = df_popularity[['averageRating', 'numVotes']]\n",
        "# use fitting because there are no particular definitive differences in sample\n",
        "model = LinearRegression(fit_intercept = True)\n",
        "# fit the data\n",
        "model.fit(X, y)\n",
        "# run the predictions\n",
        "df_popularity[['predicted_averageRating',\n",
        "               'predicted_numVotes']] = model.predict(X)"
      ],
      "metadata": {
        "id": "gK3EMxWylhdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print results of the regression and the new dataframe with the predictions\n",
        "print(\"Model intercept:\", model.intercept_)\n",
        "print(\"Model slope:    \", model.coef_[0])\n",
        "print(\"Model coeficients\", model.coef_)\n",
        "display(df_popularity)\n",
        "# save the results in a csv on Drive\n",
        "df_popularity.to_csv('/content/drive/MyDrive/regression_popularity_results.csv',\n",
        "                     encoding='utf-8')"
      ],
      "metadata": {
        "id": "CEKZORIvxRn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the plot of the regression results\n",
        "df_popularity[['averageRating', 'predicted_averageRating']].plot(alpha = 0.5);\n",
        "plt.savefig('/content/drive/MyDrive/regression_popularity_Ratings.png')"
      ],
      "metadata": {
        "id": "T8g7c3Bhn4Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the plot of the regression results\n",
        "df_popularity[['numVotes','predicted_numVotes']].plot(alpha = 0.5);\n",
        "plt.savefig('/content/drive/MyDrive/regression_popularity_Votes.png')"
      ],
      "metadata": {
        "id": "0dDiw5L6iCei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the csv in a dataframe \n",
        "df_series = pd.read_csv('/content/drive/MyDrive/series_regression.csv',\n",
        "                            header = 0, encoding='utf-8')\n",
        "# randomise the sample order\n",
        "df_series = df_series.sample(frac = 1).reset_index(drop = True)\n",
        "display(df_series)"
      ],
      "metadata": {
        "id": "MCscbH4wUqod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean the data and arrange it for analysis\n",
        "df_series = df_series.dropna(subset = None)\n",
        "# remove the string tt from the series codes if needed\n",
        "# df_series[\"seriesCode\"] = df_series[\"seriesCode\"].str.replace(\"tt\",\"\")\n",
        "# remove the trailing spaces\n",
        "df_series[\"seasonNumber\"] = df_series[\"seasonNumber\"].str.replace(r'\\\\N', '0',\n",
        "                                                                  regex = True)\n",
        "df_series[\"episodeNumber\"] = df_series[\"episodeNumber\"].str.replace(r'\\\\N', '0',\n",
        "                                                                    regex= True)\n",
        "# transform the columns from strings to integers\n",
        "df_series = df_series.astype({'seasonNumber':'int','episodeNumber':'int'})\n",
        "# sort the data\n",
        "df_series = df_series.sort_values(by=['seriesCode', 'seasonNumber',\n",
        "                                      'episodeNumber'])\n",
        "display(df_series)"
      ],
      "metadata": {
        "id": "794TTtoGlAe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the series codes csv file in a dataframe \n",
        "df_series_codes = pd.read_csv('/content/drive/MyDrive/series_codes.csv',\n",
        "                            header = 0, encoding='utf-8')\n",
        "# sort the data\n",
        "df_series_codes = df_series_codes.sort_values(by=['seriesCode'])\n",
        "# create a list with the series codes\n",
        "series_codes = df_series_codes[\"seriesCode\"].values.tolist()\n",
        "display(df_series_codes)"
      ],
      "metadata": {
        "id": "pc3X950BDkOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stereotypeNumber = [71, [1]]\n",
        "temp = df_series.loc[(df_series[\"seriesCode\"] == 'tt0040051')]\n",
        "print(temp['prediction'].sum())\n",
        "print(temp['prediction'].value_counts())\n",
        "print(temp['prediction'].value_counts()[0])\n",
        "stereotypeNumber.append(temp['prediction'].value_counts()[1] + temp['prediction'].value_counts()[0])\n",
        "stereotypeNumber.append(33)\n",
        "print(stereotypeNumber)\n",
        "display(temp[\"seasonNumber\"])"
      ],
      "metadata": {
        "id": "K0Xt-PPNk0vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stereotypeNumber = []\n",
        "episodesNumber = []\n",
        "seasonNumber = []\n",
        "for i in series_codes:\n",
        "  temp = df_series.loc[(df_series[\"seriesCode\"] == i)]\n",
        "  # stereotypeNumber.append(temp['prediction'].value_counts()[1])\n",
        "  # episodesNumber.append(len(temp))\n",
        "  # seasonNumber.append(temp[\"seasonNumber\"].max())\n",
        "  display(temp, i)"
      ],
      "metadata": {
        "id": "8UG1GBOFIjU1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}